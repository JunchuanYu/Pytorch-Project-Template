{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch lightning project template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\yujunchuan\\Anaconda\\envs\\pyt\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from argparse import ArgumentParser\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning.callbacks as plc\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from model import MInterface\n",
    "from data import DInterface\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "import importlib\n",
    "from utils import load_model_path_by_args,load_callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "# Basic Training Control\n",
    "parser.add_argument('--train_datatype', default='hdf5', type=str)\n",
    "parser.add_argument('--train_h5path', default='./dataset/train_30.hdf5', type=str)\n",
    "parser.add_argument('--test_h5path', default=None, type=str)\n",
    "parser.add_argument('--image_dir', default='./dataset/image/', type=str)\n",
    "parser.add_argument('--msk_dir', default='./dataset/mask/', type=str)\n",
    "parser.add_argument('--test_img_dir', default=None, type=str)\n",
    "parser.add_argument('--test_msk_dir', default=None, type=str)\n",
    "parser.add_argument('--split_rate', default=0.2, type=int)\n",
    "parser.add_argument('--aug', default='true', type=str)\n",
    "parser.add_argument('--stage', default='train', type=str)\n",
    "\n",
    "\n",
    "# Basic Training Control\n",
    "parser.add_argument('--input_size', default=256, type=int)\n",
    "parser.add_argument('--batch_size', default=60, type=int)\n",
    "parser.add_argument('--num_workers', default=4, type=int)\n",
    "parser.add_argument('--seed', default=42, type=int)\n",
    "parser.add_argument('--lr', default=1e-3, type=float)\n",
    "\n",
    "# LR Scheduler\n",
    "parser.add_argument('--lr_scheduler', choices='cosine', type=str)\n",
    "parser.add_argument('--lr_decay_steps', default=20, type=int)\n",
    "parser.add_argument('--lr_decay_rate', default=0.5, type=float)\n",
    "parser.add_argument('--lr_decay_min_lr', default=1e-5, type=float)\n",
    "\n",
    "# Restart Control\n",
    "parser.add_argument('--load_best', action='store_true')\n",
    "parser.add_argument('--load_dir', default=None, type=str)\n",
    "parser.add_argument('--load_ver', default=None, type=str)\n",
    "parser.add_argument('--load_v_num', default=None, type=int)\n",
    "\n",
    "# Training Info\n",
    "parser.add_argument('--dataset', default='Segh5_data', type=str)\n",
    "parser.add_argument('--data_dir', default='ref/data', type=str)\n",
    "parser.add_argument('--loss', default='bce', type=str)\n",
    "parser.add_argument('--weight_decay', default=1e-5, type=float)\n",
    "parser.add_argument('--no_augment', action='store_true')\n",
    "parser.add_argument('--log_dir', default='lightning_logs', type=str)\n",
    "\n",
    "# Model Hyperparameters\n",
    "parser.add_argument('--model_name', default='unet_vgg', type=str)\n",
    "parser.add_argument('--encoder_name', default='vgg16_bn', type=str)\n",
    "parser.add_argument('--encoder_weights', default=None, type=str)\n",
    "parser.add_argument('--activation', default=None, type=str)\n",
    "parser.add_argument('--encoder_depth', default=5, type=int)\n",
    "parser.add_argument('--class', default=2, type=int)\n",
    "\n",
    "# Other\n",
    "parser.add_argument('--aug_prob', default=0.5, type=float)\n",
    "\n",
    "# Add pytorch lightning's args to parser as a group.\n",
    "parser = Trainer.add_argparse_args(parser)\n",
    "\n",
    "## Deprecated, old version\n",
    "# parser = Trainer.add_argparse_args(\n",
    "#     parser.add_argument_group(title=\"pl.Trainer args\"))\n",
    "\n",
    "# Reset Some Default Trainer Arguments' Default Values\n",
    "parser.set_defaults(progress_bar_refresh_rate=5)\n",
    "parser.set_defaults(max_epochs=10)\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_module(name,**kwargs):\n",
    "    # Please always name your model file name as `snake_case.py` and\n",
    "    # class name corresponding `CamelCase`.\n",
    "    camel_name = ''.join([i for i in name.split('_')])\n",
    "    try:\n",
    "        data_module = getattr(importlib.import_module('data.'+name, package=__package__), camel_name)\n",
    "    except:\n",
    "        raise ValueError(\n",
    "            f'Invalid Dataset File Name or Invalid Class Name data.{name}.{camel_name}'\n",
    "            'Please always name your model file name as `snake_case.py` and class name corresponding `CamelCase`')\n",
    "    return data_module(**kwargs)\n",
    "def data_loader(args):\n",
    "    if  args.train_datatype == 'hdf5':\n",
    "        alldata = load_data_module(args.dataset,hdf5_dir=args.train_h5path,model='train',aug=True,aug_prob=0.2,args=args)\n",
    "    else: \n",
    "        alldata = load_data_module(args.dataset,image_dir=args.image_dir,msk_dir=args.msk_dir,model='train',aug=True,aug_prob=0.2,args=args)\n",
    "    trainset,valset=random_split(alldata, [int(len(alldata)*args.split_rate), len(alldata)-int(len(alldata)*args.split_rate)])\n",
    "    testset=None\n",
    "    print(len(trainset),len(valset))\n",
    "    if  args.train_datatype is 'hdf5' and args.test_h5path is not None:\n",
    "        testset = load_data_module(args.dataset,hdf5_dir=args.test_h5path,model='test',aug=True,aug_prob=0.2,args=args)\n",
    "        print(len(testset))\n",
    "    elif args.train_datatype is not 'hdf5' and args.test_img_dir is not None:\n",
    "        testset = load_data_module(args.dataset,image_dir=args.test_img_dir,msk_dir=args.test_msk_dir,model='test',aug=True,aug_prob=0.2,args=args)\n",
    "        print(len(testset))\n",
    "    data_module = DInterface(trainset,valset,testset,**vars(args))\n",
    "    return trainset,valset,testset,data_module\n",
    "def main(args):\n",
    "    pl.seed_everything(args.seed)\n",
    "    load_path = load_model_path_by_args(args)\n",
    "    trainset,valset,testset,data_module = data_loader(args)\n",
    "    if load_path is None:\n",
    "        model_module = MInterface(**vars(args))\n",
    "    else:\n",
    "        model_module = MInterface(**vars(args))\n",
    "        args.resume_from_checkpoint = load_path\n",
    "\n",
    "    logger = TensorBoardLogger(save_dir=args.log_dir)\n",
    "    args.callbacks = load_callbacks(args)\n",
    "    args.logger = logger\n",
    "    trainer=Trainer(gpus=1, max_epochs=args.max_epochs,progress_bar_refresh_rate=1,logger=args.logger,callbacks=args.callbacks)\n",
    "    trainer.fit(model_module,data_module)\n",
    "\n",
    "    # trainer = Trainer.from_argparse_args(args)\n",
    "       # from model.unetvggpl import unetvgg\n",
    "    # model=unetvgg()\n",
    "    # train_dataloader=DataLoader(trainset, batch_size=args.batch_size,  shuffle=True)\n",
    "    # val_dataloader=DataLoader(valset, batch_size=args.batch_size,  shuffle=True)\n",
    "    # trainer.fit(model,train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\yujunchuan\\Anaconda\\envs\\pyt\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name         | Type     | Params\n",
      "------------------------------------------\n",
      "0 | train_acc    | Accuracy | 0     \n",
      "1 | val_acc      | Accuracy | 0     \n",
      "2 | model_module | unetvgg  | 24.4 M\n",
      "------------------------------------------\n",
      "24.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.4 M    Total params\n",
      "97.745    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\yujunchuan\\Anaconda\\envs\\pyt\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:  50%|█████     | 1/2 [00:04<00:04,  4.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\yujunchuan\\Anaconda\\envs\\pyt\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "C:\\yujunchuan\\Anaconda\\envs\\pyt\\lib\\site-packages\\segmentation_models_pytorch\\base\\modules.py:104: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.activation(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "C:\\yujunchuan\\Anaconda\\envs\\pyt\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "C:\\yujunchuan\\Anaconda\\envs\\pyt\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:433: UserWarning: The number of training samples (17) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|▍         | 4/84 [00:06<02:01,  1.52s/it, loss=99.9, v_num=15, train_loss=99.90, train_acc=0.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\yujunchuan\\Anaconda\\envs\\pyt\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d92b5224e6d87ceec00c4f9df6c9e2713e512c7988f636471a091b7344275304"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pyt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
